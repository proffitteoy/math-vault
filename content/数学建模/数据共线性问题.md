
## **数据共线性 (Collinearity) 问题**

>[!tip]
>在一般的线性模型中，共线性问题并非一个统计问题，而是一个纯粹的**代数结构问题 (Algebraic Structure Issue)**。其本质在于，模型中的解释变量所构成的列向量集合，未能在一个$n$维向量空间中张成一个$p$维的子空间，即这些向量是**线性相关**的。这直接导致了模型参数的解不再唯一，或在近似情况下表现出极高的方差和不稳定性。

### 1. 共线性的结构性定义
>[!定义1]
>给定设计矩阵$X \in \mathbb{R}^{n \times p}$，其列向量为解释变量$\{X_1, X_2, \dots, X_p\}$。若存在一个**非零向量**$\alpha = (\alpha_1, \dots, \alpha_p)^\top \in \mathbb{R}^p$使得
>$$X\alpha = 0$$
>则称矩阵$X$的列向量存在**（完全）共线性**。

这一定义等价于说，列向量集合$\{X_1, \dots, X_p\}$是**线性相关 (Linearly Dependent)** 的。其直接的代数后果是，矩阵$X$的秩小于其列数，即
$$\operatorname{rank}(X) < p$$

### 2. 共线性引发的结构性问题
#### 2.1 参数的不可辨识性 (Unidentifiability)
在线性回归模型$Y = X\beta + \varepsilon$中，其普通最小二乘 (OLS) 估计量由正规方程$X^\top X \hat{\beta} = X^\top Y$给出。若$X^\top X$可逆，则
$$\hat{\beta} = (X^\top X)^{-1}X^\top Y$$
当$X$存在共线性时，$\operatorname{rank}(X) < p$，则
$$\operatorname{rank}(X^\top X) = \operatorname{rank}(X) < p$$
此时矩阵$X^\top X$是**奇异的 (Singular)**，即**不可逆 (Non-invertible)**。这意味着$(X^\top X)^{-1}$**不存在**。
因此，参数$\beta$的解不再是唯一的。任意两个解$\beta_1$和$\beta_2$都满足$X\beta_1 = X\beta_2$，其差向量$\beta_1 - \beta_2$位于$X$的零空间 (Null Space) 中。解的集合构成了一个**仿射子空间 (Affine Subspace)**。

#### 2.2 估计量的高方差 (High Variance)
当共线性不是完全的，而是**近似共线性 (Near Collinearity)** 时，列向量近似线性相关。在代数结构上，这表现为矩阵$X^\top X$的最小特征值$\lambda_{\min}$非常接近于零：
$$\lambda_{\min}(X^\top X) \approx 0$$
此时$X^\top X$虽然可逆，但其逆矩阵的谱范数会极大。考虑到OLS估计量的方差-协方差矩阵为
$$\operatorname{Var}(\hat{\beta}) = \sigma^2(X^\top X)^{-1}$$
由于$(X^\top X)^{-1}$的特征值为$1/\lambda_i(X^\top X)$，其最大特征值$1/\lambda_{\min}$会非常大，导致$\hat{\beta}$的某些线性组合的方差极大。

#### 2.3 估计的不稳定性 (Instability)
近似共线性也意味着矩阵$X^\top X$是**病态的 (Ill-conditioned)**，即其条件数
$$\kappa(X^\top X) = \frac{\lambda_{\max}(X^\top X)}{\lambda_{\min}(X^\top X)} \gg 1$$
这使得求解过程对数据$X$或$Y$的微小扰动极为敏感。任何小的扰动$\Delta X$或$\Delta Y$都会通过范数巨大的逆矩阵$(X^\top X)^{-1}$被放大，从而导致估计向量$\hat{\beta}$发生剧烈变化。

### 3. 解决方案的代数思想
#### 3.1 移除相关方向 (Variable Selection)
此方法旨在通过剔除某些列向量，使得留下的列向量集合是线性无关的，即恢复设计矩阵的满秩性质。
代数上，即找到满足$X\alpha=0$的$\alpha$，并移除其非零分量对应的列。

#### 3.2 正则化 (Regularization)
以岭回归 (Ridge Regression) 为例，其目标函数为$\min_{\beta} ||Y-X\beta||_2^2 + \lambda||\beta||_2^2$。其解为
$$\hat{\beta}_{ridge} = (X^\top X + \lambda I)^{-1}X^\top Y$$
从结构上看，通过给$X^\top X$加上一个对角矩阵$\lambda I$，使得新的矩阵$(X^\top X + \lambda I)$的最小特征值
$$\lambda_{\min}(X^\top X + \lambda I) = \lambda_{\min}(X^\top X) + \lambda \geq \lambda > 0$$
从而保证了矩阵的可逆性并改善了其条件数，使得求解过程变得稳定。

#### 3.3 变量重构 (Variable Reconstruction)
此方法旨在保持原有信息的同时，重构一组新的解释变量，使它们在代数上是**正交的 (Orthogonal)**。
主成分分析 (PCA) 是典型方法，它通过对$X^\top X$进行特征分解，找到一组正交基来表示原有的列空间，从根本上消除了变量间的线性相关性。


