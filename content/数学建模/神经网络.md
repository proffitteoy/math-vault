
## 1. 机器学习回归与分类

**核心思想**：
利用带标签的数据，通过损失函数（如 MSE、交叉熵）最小化，学习映射
$$
f: X \to y
$$
用于预测连续变量（回归）或类别标签（分类）。

**常见模型**：

* 决策树、随机森林、XGBoost
* 线性回归、岭回归、Lasso
* SVM
* 神经网络（BP神经网络、LSTM等也是其中一类）

**适用场景**：所有监督学习问题。


## 2. Logit 回归（逻辑斯蒂回归）

**核心思想**：
用逻辑函数将线性组合映射到 (0,1)，预测概率：
$$
p = \sigma(w^T x + b)
$$
然后通过最大似然估计训练参数。

**本质**：

* 不是“回归”，而是一个线性分类模型 + 概率化输出
* 所有线性可分问题最基础、可解释性最强

**用途**：

* 分类基线模型（baseline）
* 可解释权重（哪个变量影响分类概率）


## 3. BP 神经网络
**核心思想**：
一个前馈神经网络：
$$
\text{FC} \to \text{Activation} \to \text{FC} \to \text{Activation}
$$
通过反向传播（链式法则）更新参数。

**关键点**：

* 神经网络的**最核心底层机制**是 BP
* 所有深度学习模型本质都依赖 BP（包括 LSTM、CNN、Transformer）

**适用场景**：

* 低维数据拟合（回归、分类）
* 结构简单的问题（例如建模题中的预测任务）

## 4. LSTM 神经网络（长短期记忆）

**核心思想**：
为解决 RNN 的“梯度消失”问题，引入门结构：
$$
\text{forget gate},\ \text{input gate},\ \text{output gate}
$$
通过加入 cell state 记忆长期依赖。

**适用场景**：

* 时间序列预测（风速、流量、销量、股价）
* 文本序列建模（较旧任务，现在多被 Transformer 替代）

**在建模比赛中的典型任务**：预测未来某指标。


## 5. XGBoost 回归

**核心思想**：
通过加法模型累积弱学习器（树）：
$$
\hat{y} = \sum_{k=1}^K f_k(x)
$$
每棵树拟合上一轮的残差。

它是性能最强的传统 ML 模型之一（tabular data 的王者）。

**优势**：

* 在小数据、表格数据中远强于神经网络
* 无需大量调参
* 很难 overfitting（正则项较强）

**用途**：

* 预测任务 baseline
* 建模比赛中广泛使用



## 都属于监督学习架构

无论是 Logit、BP 神经网络还是 LSTM，都在用同一个框架：

* 定义损失函数
* 计算梯度
* 梯度下降更新参数

**区别只是模型结构不同**。



## 2. BP 是所有深度学习的核心算法

* Logit：无隐层的网络
* BP 网络：多层感知机
* LSTM：带门结构的 RNN
* Transformer：带 attention 的深层网络
  → 都是 BP 推导的结果。


# 三、数学建模比赛中的“使用策略”

| 任务类型      | 最适合方法                 | 次选方法            |
| --------- | --------------------- | --------------- |
| 表格数据预测    | **XGBoost**           | BP、Logit        |
| 时间序列预测    | **LSTM**              | ARIMA、BP        |
| 文本分类      | **TF-IDF + XGBoost**  | LSTM            |
| 分类任务（结构化） | **Logit/树模型/XGBoost** | BP              |
| 非线性拟合     | **BP神经网络**            | XGBoost         |
| 复杂序列建模    | **LSTM**              | GRU、Transformer |
