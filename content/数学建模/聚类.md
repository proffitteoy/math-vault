
| 算法类别      | 代表算法                                  | 数学核心思想                 | 典型优点          | 局限性          | 适用场景         |
| --------- | ------------------------------------- | ---------------------- | ------------- | ------------ | ------------ |
| **划分类**   | K-Means                               | 欧氏距离 + 最小化平方误差         | 快速、简单         | 只适合球状分布、K需先定 | 样本集中、维度较低的任务 |
| **概率类**   | GMM (高斯混合模型)                          | 最大似然估计 + EM算法          | 能表达非球状分布、概率输出 | 对初值敏感、易陷局部最优 | 聚类不均匀、需概率解释时 |
| **密度类**   | DBSCAN / LOF                          | 邻域密度估计 + 局部可达密度        | 发现任意形状簇、抗噪性强  | 参数敏感、难处理高维数据 | 空间数据、异常点检测   |
| **降维类**   | LDA降维 (线性判别分析)                        | Fisher判别准则：类间方差最大、类内最小 | 有监督降维         | 需类别标签、线性假设   | 降维预处理、特征提取   |
| **主题类**   | LDA主题模型 (Latent Dirichlet Allocation) | 概率生成模型 + Dirichlet先验   | 能解释文档-主题-词关系  | 计算复杂、需文本数据   | 文本聚类、主题提取    |
| **异常检测类** | LOF (Local Outlier Factor)            | 局部密度比值                 | 发现局部异常点       | 参数需调、不可扩展性差  | 异常检测、网络入侵检测  |

K-Means是GMM的特例（GMM 中若假设每个高斯成分方差相同且为球形，EM算法退化为K-Means）。
LOF 是 DBSCAN 的“局部化版本”，将密度概念从绝对改为相对。
LDA降维或PCA常用于聚类前的特征压缩，以降低维度、去噪。

DBSCAN:https://scikit-learn.org/stable/modules/clustering.html#dbscan
除了LDA降维还可以试试别的降维方法：https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/





| 方法                                                | 适用场景                            | 优点                   | 限制                                               |
| ------------------------------------------------- | ------------------------------- | -------------------- | ------------------------------------------------ |
| **Low Variance Filter / High Correlation Filter** | 特征数很多、存在明显冗余或常数特征场景             | 实现简单、速度快             | 只是粗过滤；可能删掉有用但低方差或低相关特征                           |
| **PCA**                                           | 希望做线性降维、准备可视化、或者压缩/预处理特征        | 线性、易解释、可保最大方差        | 假设线性关系；投影后的特征可能无法解释原始含义；对异常和尺度敏感                 |
| **ICA**                                           | 数据构成是多个独立源混合（如信号分离、盲源分离）        | 找“独立”成分、适合混合信号       | 对噪声敏感；需要假设独立性；降维目的不如PCA直接                        |
| **t-SNE**                                         | 高维可视化（2-3维）、探索数据结构              | 擅长局部结构展示、聚类/分群可视化效果好 | 对大型数据慢；无法很好保留全局结构，结果随机性强，难以用于特征预处理再进入模型（更多用于可视化） |
| **UMAP**                                          | 同样用于可视化／降维，但希望兼顾局部与全局结构、数据规模较大时 | 较快、可扩展、结构保留较好        | 参数多、结果解释困难、更多用于探索而非全自动建模流程                       |
| **Factor Analysis**                               | 背后假设存在少数潜在因子（例如社会科学、心理学问卷数据）    | 可解释性较强（因子可命名）        | 适用假设较多；对非正态/非线性结构限制较大                            |

除此之外还可以尝试特征选择，这是另外一个主题了
